---
title: "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling" 
collection: publications
permalink: /publication/cab
excerpt: 'CAB aims to provide a comprehensive evaluation of efficient attentions. CAB contains seven real-world tasks from different research areas to evaluate efficient attentions under four fine-grained attention patterns.'
date: 2022-11-18
venue: 'ICLR'
paperurl: 'https://arxiv.org/pdf/2210.07661.pdf'
citation: '@article{zhang2022cab,
  title={CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  author={Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2210.07661},
  year={2022}
}'
---
This paper is about the number 1. The number 2 is left for future work.

[Download paper here](https://arxiv.org/pdf/2210.07661.pdf)

Recommended citation:   
```bibtex
@article{zhang2022cab,
  title={CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  author={Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2210.07661},
  year={2022}
}
```