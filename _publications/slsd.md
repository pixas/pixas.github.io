---
title: "Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation" 
collection: publications
permalink: /publication/slsd
excerpt: 'SLSD self-distilled outputs of non-autoregressive models to mitigate the multi-modality problems of non-autoregressive machine translation'
date: 2023-12-6
venue: 'EMNLP'
paperurl: 'https://aclanthology.org/2023.emnlp-main.878.pdf'
citation: '
```
@inproceedings{liao2023self,
  title={Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation},
  author={Liao, Yusheng and Jiang, Shuyang and Li, Yiqi and Wang, Yu and Wang, Yanfeng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14202--14212},
  year={2023}
}
```
'
---


[Download paper here](https://aclanthology.org/2023.emnlp-main.878.pdf)

Recommended citation:   
```bibtex
@inproceedings{liao2023self,
  title={Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation},
  author={Liao, Yusheng and Jiang, Shuyang and Li, Yiqi and Wang, Yu and Wang, Yanfeng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14202--14212},
  year={2023}
}
```